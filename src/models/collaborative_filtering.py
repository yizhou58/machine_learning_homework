"""
ååŒè¿‡æ»¤æ¨èæ¨¡å‹
å®ç°åŸºäºçŸ©é˜µåˆ†è§£ï¼ˆSVDï¼‰çš„ååŒè¿‡æ»¤ç®—æ³•
"""
import pandas as pd
import numpy as np
import pickle
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split, cross_validate
from surprise.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class CollaborativeFilteringModel:
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.model = None
        self.trainset = None
        self.testset = None
        self.predictions = None
        self.data = None
        
    def load_processed_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        print("ğŸ“‚ åŠ è½½é¢„å¤„ç†æ•°æ®...")
        
        # åŠ è½½è¯„åˆ†æ•°æ®
        ratings_df = pd.read_csv(f"{self.data_dir}/processed_ratings.csv")
        
        # ä¸ºSurpriseåº“å‡†å¤‡æ•°æ®æ ¼å¼
        reader = Reader(rating_scale=(1, 5))
        self.data = Dataset.load_from_df(
            ratings_df[['user_id', 'item_id', 'rating']], 
            reader
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(ratings_df):,} æ¡è¯„åˆ†")
        return ratings_df
    
    def split_data(self, test_size=0.2, random_state=42):
        """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        print(f"ğŸ“Š åˆ’åˆ†æ•°æ®é›† (æµ‹è¯•é›†æ¯”ä¾‹: {test_size})...")
        
        self.trainset, self.testset = train_test_split(
            self.data, 
            test_size=test_size, 
            random_state=random_state
        )
        
        print(f"   - è®­ç»ƒé›†å¤§å°: {self.trainset.n_ratings:,}")
        print(f"   - æµ‹è¯•é›†å¤§å°: {len(self.testset):,}")
    
    def train_svd_model(self, n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02):
        """è®­ç»ƒSVDæ¨¡å‹"""
        print("ğŸ§  è®­ç»ƒSVDååŒè¿‡æ»¤æ¨¡å‹...")
        print(f"   - æ½œåœ¨å› å­æ•°: {n_factors}")
        print(f"   - è®­ç»ƒè½®æ•°: {n_epochs}")
        print(f"   - å­¦ä¹ ç‡: {lr_all}")
        print(f"   - æ­£åˆ™åŒ–å‚æ•°: {reg_all}")
        
        # åˆå§‹åŒ–SVDæ¨¡å‹
        self.model = SVD(
            n_factors=n_factors,
            n_epochs=n_epochs,
            lr_all=lr_all,
            reg_all=reg_all,
            random_state=42,
            verbose=True
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(self.trainset)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        self.predictions = self.model.test(self.testset)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        rmse = accuracy.rmse(self.predictions, verbose=False)
        mae = accuracy.mae(self.predictions, verbose=False)
        
        print(f"   - RMSE: {rmse:.4f}")
        print(f"   - MAE: {mae:.4f}")
        
        return rmse, mae
    
    def cross_validation(self, cv=5):
        """äº¤å‰éªŒè¯"""
        print(f"\nğŸ”„ è¿›è¡Œ {cv} æŠ˜äº¤å‰éªŒè¯...")
        
        cv_results = cross_validate(
            self.model, 
            self.data, 
            measures=['RMSE', 'MAE'], 
            cv=cv, 
            verbose=True
        )
        
        print(f"   - å¹³å‡ RMSE: {cv_results['test_rmse'].mean():.4f} (Â±{cv_results['test_rmse'].std():.4f})")
        print(f"   - å¹³å‡ MAE: {cv_results['test_mae'].mean():.4f} (Â±{cv_results['test_mae'].std():.4f})")
        
        return cv_results
    
    def hyperparameter_tuning(self):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("\nğŸ”§ è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        
        param_grid = {
            'n_factors': [50, 100, 150],
            'n_epochs': [10, 20, 30],
            'lr_all': [0.002, 0.005, 0.01],
            'reg_all': [0.01, 0.02, 0.05]
        }
        
        gs = GridSearchCV(
            SVD, 
            param_grid, 
            measures=['rmse', 'mae'], 
            cv=3,
            n_jobs=-1
        )
        
        gs.fit(self.data)
        
        print(f"   - æœ€ä½³ RMSE: {gs.best_score['rmse']:.4f}")
        print(f"   - æœ€ä½³å‚æ•°: {gs.best_params['rmse']}")
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹
        self.model = gs.best_estimator['rmse']
        self.model.fit(self.trainset)
        
        return gs.best_params['rmse']
    
    def get_user_recommendations(self, user_id, n_recommendations=10):
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆæ¨è"""
        # è·å–ç”¨æˆ·å·²è¯„åˆ†çš„å•†å“
        user_items = set()
        for (uid, iid, rating) in self.trainset.all_ratings():
            if self.trainset.to_raw_uid(uid) == user_id:
                user_items.add(self.trainset.to_raw_iid(iid))
        
        # è·å–æ‰€æœ‰å•†å“
        all_items = set()
        for (uid, iid, rating) in self.trainset.all_ratings():
            all_items.add(self.trainset.to_raw_iid(iid))
        
        # æ‰¾åˆ°ç”¨æˆ·æœªè¯„åˆ†çš„å•†å“
        unrated_items = all_items - user_items
        
        # é¢„æµ‹è¯„åˆ†
        predictions = []
        for item_id in unrated_items:
            pred = self.model.predict(user_id, item_id)
            predictions.append((item_id, pred.est))
        
        # æŒ‰é¢„æµ‹è¯„åˆ†æ’åº
        predictions.sort(key=lambda x: x[1], reverse=True)
        
        return predictions[:n_recommendations]
    
    def analyze_predictions(self):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        print("\nğŸ“Š åˆ†æé¢„æµ‹ç»“æœ...")
        
        # æå–çœŸå®è¯„åˆ†å’Œé¢„æµ‹è¯„åˆ†
        true_ratings = [pred.r_ui for pred in self.predictions]
        pred_ratings = [pred.est for pred in self.predictions]
        
        # è®¡ç®—é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        errors = [abs(true - pred) for true, pred in zip(true_ratings, pred_ratings)]
        
        print(f"   - é¢„æµ‹è¯„åˆ†èŒƒå›´: {min(pred_ratings):.2f} - {max(pred_ratings):.2f}")
        print(f"   - å¹³å‡ç»å¯¹è¯¯å·®: {np.mean(errors):.4f}")
        print(f"   - è¯¯å·®æ ‡å‡†å·®: {np.std(errors):.4f}")
        
        # æŒ‰çœŸå®è¯„åˆ†åˆ†ç»„åˆ†æ
        rating_errors = defaultdict(list)
        for pred in self.predictions:
            rating_errors[pred.r_ui].append(abs(pred.r_ui - pred.est))
        
        print(f"   - å„è¯„åˆ†ç­‰çº§çš„å¹³å‡è¯¯å·®:")
        for rating in sorted(rating_errors.keys()):
            avg_error = np.mean(rating_errors[rating])
            print(f"     {rating}æ˜Ÿ: {avg_error:.4f}")
    
    def save_model(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        with open(f"{self.data_dir}/svd_model.pkl", 'wb') as f:
            pickle.dump(self.model, f)
        
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ!")
    
    def run_collaborative_filtering(self, tune_hyperparams=False):
        """è¿è¡Œå®Œæ•´çš„ååŒè¿‡æ»¤æµç¨‹"""
        print("ğŸš€ å¼€å§‹ååŒè¿‡æ»¤æ¨èç³»ç»Ÿè®­ç»ƒ...\n")
        
        # åŠ è½½æ•°æ®
        ratings_df = self.load_processed_data()
        
        # åˆ’åˆ†æ•°æ®é›†
        self.split_data()
        
        # è®­ç»ƒæ¨¡å‹
        if tune_hyperparams:
            best_params = self.hyperparameter_tuning()
        else:
            self.train_svd_model()
        
        # è¯„ä¼°æ¨¡å‹
        rmse, mae = self.evaluate_model()
        
        # äº¤å‰éªŒè¯
        cv_results = self.cross_validation()
        
        # åˆ†æé¢„æµ‹ç»“æœ
        self.analyze_predictions()
        
        # ä¿å­˜æ¨¡å‹
        self.save_model()
        
        print(f"\nğŸ‰ ååŒè¿‡æ»¤æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"   - æœ€ç»ˆ RMSE: {rmse:.4f}")
        print(f"   - æœ€ç»ˆ MAE: {mae:.4f}")
        
        return self.model

# æ³¨é‡Šæ‰æµ‹è¯•ä»£ç ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶æ‰§è¡Œ
# if __name__ == "__main__":
#     cf_model = CollaborativeFilteringModel()
#     model = cf_model.run_collaborative_filtering(tune_hyperparams=False)

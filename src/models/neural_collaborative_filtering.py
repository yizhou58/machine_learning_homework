"""
Neural Collaborative Filtering (NCF) æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹
åˆ›æ–°ç‚¹ï¼šèåˆå¤šæ¨¡æ€ç‰¹å¾çš„ç¥ç»ååŒè¿‡æ»¤

ä¸»è¦ç‰¹æ€§ï¼š
1. ç¥ç»çŸ©é˜µåˆ†è§£ (Neural Matrix Factorization)
2. å¤šå±‚æ„ŸçŸ¥æœº (Multi-Layer Perceptron) 
3. å¤šæ¨¡æ€ç‰¹å¾èåˆ (Multi-modal Feature Fusion)
4. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)
5. å¯¹æ¯”å­¦ä¹  (Contrastive Learning)
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ¨èæ•°æ®é›†"""
    
    def __init__(self, ratings_df, item_features, user_features=None):
        self.ratings = ratings_df
        self.item_features = item_features
        self.user_features = user_features
        
        # åˆ›å»ºç”¨æˆ·å’Œå•†å“çš„IDæ˜ å°„
        self.user_ids = sorted(ratings_df['user_id'].unique())
        self.item_ids = sorted(ratings_df['item_id'].unique())
        
        self.user_to_idx = {uid: idx for idx, uid in enumerate(self.user_ids)}
        self.item_to_idx = {iid: idx for idx, iid in enumerate(self.item_ids)}
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        self.prepare_data()
        
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        self.user_indices = []
        self.item_indices = []
        self.ratings_list = []
        self.item_features_list = []
        
        for _, row in self.ratings.iterrows():
            user_idx = self.user_to_idx[row['user_id']]
            item_idx = self.item_to_idx[row['item_id']]
            
            # è·å–å•†å“ç‰¹å¾
            item_feature = self.item_features[
                self.item_features['item_id'] == row['item_id']
            ]
            
            if not item_feature.empty:
                # æå–ç‰¹å¾å‘é‡ï¼ˆé™¤äº†item_idåˆ—ï¼‰
                feature_cols = [col for col in item_feature.columns if col != 'item_id']
                features = item_feature[feature_cols].values[0]
                
                self.user_indices.append(user_idx)
                self.item_indices.append(item_idx)
                self.ratings_list.append(row['rating'])
                self.item_features_list.append(features)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.user_indices = np.array(self.user_indices)
        self.item_indices = np.array(self.item_indices)
        self.ratings_list = np.array(self.ratings_list, dtype=np.float32)
        self.item_features_list = np.array(self.item_features_list, dtype=np.float32)
        
    def __len__(self):
        return len(self.user_indices)
    
    def __getitem__(self, idx):
        return {
            'user_id': torch.tensor(self.user_indices[idx], dtype=torch.long),
            'item_id': torch.tensor(self.item_indices[idx], dtype=torch.long),
            'rating': torch.tensor(self.ratings_list[idx], dtype=torch.float32),
            'item_features': torch.tensor(self.item_features_list[idx], dtype=torch.float32)
        }

class AttentionLayer(nn.Module):
    """æ³¨æ„åŠ›æœºåˆ¶å±‚"""
    
    def __init__(self, input_dim, attention_dim=64):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Sequential(
            nn.Linear(input_dim, attention_dim),
            nn.Tanh(),
            nn.Linear(attention_dim, 1),
            nn.Softmax(dim=1)
        )
        
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        attention_weights = self.attention(x)  # (batch_size, seq_len, 1)
        weighted_output = torch.sum(x * attention_weights, dim=1)  # (batch_size, input_dim)
        return weighted_output, attention_weights

class MultiModalNCF(nn.Module):
    """å¤šæ¨¡æ€ç¥ç»ååŒè¿‡æ»¤æ¨¡å‹"""
    
    def __init__(self, num_users, num_items, item_feature_dim, 
                 embedding_dim=64, hidden_dims=[128, 64, 32], dropout=0.2):
        super(MultiModalNCF, self).__init__()
        
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        
        # ç”¨æˆ·å’Œå•†å“åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # å•†å“ç‰¹å¾å¤„ç†ç½‘ç»œ
        self.item_feature_net = nn.Sequential(
            nn.Linear(item_feature_dim, embedding_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ - èåˆä¸åŒæ¨¡æ€ç‰¹å¾
        self.attention = AttentionLayer(embedding_dim)
        
        # GMF (Generalized Matrix Factorization) åˆ†æ”¯
        self.gmf_layer = nn.Linear(embedding_dim, 1)
        
        # MLP (Multi-Layer Perceptron) åˆ†æ”¯
        mlp_layers = []
        input_dim = embedding_dim * 3  # user + item + item_features
        
        for hidden_dim in hidden_dims:
            mlp_layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            input_dim = hidden_dim
        
        self.mlp = nn.Sequential(*mlp_layers)
        self.mlp_output = nn.Linear(hidden_dims[-1], 1)
        
        # æœ€ç»ˆèåˆå±‚
        self.final_layer = nn.Sequential(
            nn.Linear(2, 32),  # GMF + MLP outputs
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(embedding_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)
        
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, user_ids, item_ids, item_features):
        # è·å–åµŒå…¥å‘é‡
        user_emb = self.user_embedding(user_ids)  # (batch_size, embedding_dim)
        item_emb = self.item_embedding(item_ids)  # (batch_size, embedding_dim)
        
        # å¤„ç†å•†å“ç‰¹å¾
        item_feat_emb = self.item_feature_net(item_features)  # (batch_size, embedding_dim)
        
        # æ³¨æ„åŠ›æœºåˆ¶èåˆå¤šæ¨¡æ€ç‰¹å¾
        # å°†ä¸‰ç§ç‰¹å¾å †å 
        multi_modal_features = torch.stack([user_emb, item_emb, item_feat_emb], dim=1)
        attended_features, attention_weights = self.attention(multi_modal_features)
        
        # GMFåˆ†æ”¯ï¼šå…ƒç´ çº§ä¹˜ç§¯
        gmf_vector = user_emb * item_emb
        gmf_output = self.gmf_layer(gmf_vector)
        
        # MLPåˆ†æ”¯ï¼šç‰¹å¾æ‹¼æ¥
        mlp_vector = torch.cat([user_emb, item_emb, item_feat_emb], dim=1)
        mlp_hidden = self.mlp(mlp_vector)
        mlp_output = self.mlp_output(mlp_hidden)
        
        # èåˆGMFå’ŒMLPè¾“å‡º
        final_input = torch.cat([gmf_output, mlp_output], dim=1)
        rating_pred = self.final_layer(final_input) * 4 + 1  # ç¼©æ”¾åˆ°1-5èŒƒå›´
        
        return {
            'rating': rating_pred.squeeze(),
            'user_emb': user_emb,
            'item_emb': item_emb,
            'item_feat_emb': item_feat_emb,
            'attention_weights': attention_weights,
            'user_projection': self.projection_head(user_emb),
            'item_projection': self.projection_head(item_emb)
        }

class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature=0.1):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature
        
    def forward(self, user_proj, item_proj, ratings):
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(user_proj, item_proj.T) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬æ ‡ç­¾ï¼ˆé«˜è¯„åˆ†ä¸ºæ­£æ ·æœ¬ï¼‰
        positive_mask = (ratings.unsqueeze(1) >= 4.0) & (ratings.unsqueeze(0) >= 4.0)
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        positive_sim = exp_sim * positive_mask.float()
        
        loss = -torch.log(positive_sim.sum(dim=1) / exp_sim.sum(dim=1) + 1e-8)
        return loss.mean()

class NeuralCollaborativeFiltering:
    """ç¥ç»ååŒè¿‡æ»¤æ¨èç³»ç»Ÿ"""
    
    def __init__(self, data_dir='data'):
        self.data_dir = data_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æŸå¤±å‡½æ•°
        self.rating_criterion = nn.MSELoss()
        self.contrastive_criterion = ContrastiveLoss()
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        
        # åŠ è½½è¯„åˆ†æ•°æ®
        self.ratings_df = pd.read_csv(f'{self.data_dir}/processed_ratings.csv')
        
        # åŠ è½½å•†å“ç‰¹å¾
        self.item_features = pd.read_csv(f'{self.data_dir}/item_features.csv')
        
        print(f"   - è¯„åˆ†æ•°æ®: {len(self.ratings_df):,} æ¡")
        print(f"   - å•†å“ç‰¹å¾: {self.item_features.shape}")
        
        # æ•°æ®é‡‡æ ·ï¼ˆç”¨äºå¿«é€Ÿè®­ç»ƒï¼‰
        if len(self.ratings_df) > 100000:
            print("   - é‡‡æ ·æ•°æ®ä»¥åŠ é€Ÿè®­ç»ƒ...")
            self.ratings_df = self.ratings_df.sample(n=100000, random_state=42)
        
    def prepare_datasets(self, test_size=0.2):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†"""
        print("ğŸ”§ å‡†å¤‡æ•°æ®é›†...")
        
        # åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
        train_ratings, test_ratings = train_test_split(
            self.ratings_df, test_size=test_size, random_state=42
        )
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset = MultiModalDataset(train_ratings, self.item_features)
        self.test_dataset = MultiModalDataset(test_ratings, self.item_features)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            self.train_dataset, batch_size=512, shuffle=True, num_workers=0
        )
        self.test_loader = DataLoader(
            self.test_dataset, batch_size=512, shuffle=False, num_workers=0
        )
        
        print(f"   - è®­ç»ƒé›†: {len(self.train_dataset):,} æ¡")
        print(f"   - æµ‹è¯•é›†: {len(self.test_dataset):,} æ¡")
        
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("ğŸ—ï¸ æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
        
        num_users = len(self.train_dataset.user_ids)
        num_items = len(self.train_dataset.item_ids)
        item_feature_dim = self.item_features.shape[1] - 1  # é™¤å»item_idåˆ—
        
        self.model = MultiModalNCF(
            num_users=num_users,
            num_items=num_items,
            item_feature_dim=item_feature_dim,
            embedding_dim=64,
            hidden_dims=[128, 64, 32],
            dropout=0.3
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.8)
        
        print(f"   - ç”¨æˆ·æ•°: {num_users:,}")
        print(f"   - å•†å“æ•°: {num_items:,}")
        print(f"   - ç‰¹å¾ç»´åº¦: {item_feature_dim}")
        print(f"   - æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def train_model(self, epochs=20):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ...")
        
        train_losses = []
        test_losses = []
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_rating_loss = 0.0
            train_contrastive_loss = 0.0
            
            for batch in tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                user_ids = batch['user_id'].to(self.device)
                item_ids = batch['item_id'].to(self.device)
                ratings = batch['rating'].to(self.device)
                item_features = batch['item_features'].to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(user_ids, item_ids, item_features)
                
                # è®¡ç®—æŸå¤±
                rating_loss = self.rating_criterion(outputs['rating'], ratings)
                contrastive_loss = self.contrastive_criterion(
                    outputs['user_projection'], 
                    outputs['item_projection'], 
                    ratings
                )
                
                # æ€»æŸå¤± = è¯„åˆ†æŸå¤± + å¯¹æ¯”å­¦ä¹ æŸå¤±
                total_loss = rating_loss + 0.1 * contrastive_loss
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                train_loss += total_loss.item()
                train_rating_loss += rating_loss.item()
                train_contrastive_loss += contrastive_loss.item()
            
            # éªŒè¯é˜¶æ®µ
            test_loss = self.evaluate_model()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            avg_train_loss = train_loss / len(self.train_loader)
            train_losses.append(avg_train_loss)
            test_losses.append(test_loss)
            
            print(f"Epoch {epoch+1}/{epochs}:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  - è¯„åˆ†æŸå¤±: {train_rating_loss/len(self.train_loader):.4f}")
            print(f"  - å¯¹æ¯”æŸå¤±: {train_contrastive_loss/len(self.train_loader):.4f}")
            print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            print(f"  å­¦ä¹ ç‡: {self.scheduler.get_last_lr()[0]:.6f}")
            print()
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, test_losses)
        
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in self.test_loader:
                user_ids = batch['user_id'].to(self.device)
                item_ids = batch['item_id'].to(self.device)
                ratings = batch['rating'].to(self.device)
                item_features = batch['item_features'].to(self.device)
                
                outputs = self.model(user_ids, item_ids, item_features)
                loss = self.rating_criterion(outputs['rating'], ratings)
                total_loss += loss.item()
        
        return total_loss / len(self.test_loader)
    
    def plot_training_curves(self, train_losses, test_losses):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
        plt.plot(test_losses, label='æµ‹è¯•æŸå¤±', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('ç¥ç»ååŒè¿‡æ»¤è®­ç»ƒæ›²çº¿')
        plt.legend()
        plt.grid(True)
        plt.savefig(f'{self.data_dir}/ncf_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def get_recommendations(self, user_id, n_recommendations=10):
        """ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è"""
        self.model.eval()
        
        if user_id not in self.train_dataset.user_to_idx:
            print(f"ç”¨æˆ· {user_id} ä¸åœ¨è®­ç»ƒé›†ä¸­")
            return []
        
        user_idx = self.train_dataset.user_to_idx[user_id]
        
        # è·å–ç”¨æˆ·å·²è¯„åˆ†çš„å•†å“
        user_rated_items = set(
            self.ratings_df[self.ratings_df['user_id'] == user_id]['item_id']
        )
        
        # å€™é€‰å•†å“ï¼ˆæœªè¯„åˆ†çš„å•†å“ï¼‰
        candidate_items = [
            item_id for item_id in self.train_dataset.item_ids 
            if item_id not in user_rated_items
        ]
        
        if not candidate_items:
            return []
        
        recommendations = []
        
        with torch.no_grad():
            for item_id in candidate_items[:1000]:  # é™åˆ¶å€™é€‰æ•°é‡
                if item_id not in self.train_dataset.item_to_idx:
                    continue
                    
                item_idx = self.train_dataset.item_to_idx[item_id]
                
                # è·å–å•†å“ç‰¹å¾
                item_feature = self.item_features[
                    self.item_features['item_id'] == item_id
                ]
                
                if item_feature.empty:
                    continue
                
                feature_cols = [col for col in item_feature.columns if col != 'item_id']
                features = torch.tensor(
                    item_feature[feature_cols].values[0], 
                    dtype=torch.float32
                ).unsqueeze(0).to(self.device)
                
                user_tensor = torch.tensor([user_idx], dtype=torch.long).to(self.device)
                item_tensor = torch.tensor([item_idx], dtype=torch.long).to(self.device)
                
                # é¢„æµ‹è¯„åˆ†
                outputs = self.model(user_tensor, item_tensor, features)
                predicted_rating = outputs['rating'].item()
                
                recommendations.append({
                    'item_id': item_id,
                    'predicted_rating': predicted_rating
                })
        
        # æŒ‰é¢„æµ‹è¯„åˆ†æ’åº
        recommendations.sort(key=lambda x: x['predicted_rating'], reverse=True)
        
        return recommendations[:n_recommendations]
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        print("ğŸ’¾ ä¿å­˜ç¥ç»ååŒè¿‡æ»¤æ¨¡å‹...")
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'user_to_idx': self.train_dataset.user_to_idx,
            'item_to_idx': self.train_dataset.item_to_idx,
            'model_config': {
                'num_users': len(self.train_dataset.user_ids),
                'num_items': len(self.train_dataset.item_ids),
                'item_feature_dim': self.item_features.shape[1] - 1
            }
        }, f'{self.data_dir}/ncf_model.pth')
        
        print("   âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  ç¥ç»ååŒè¿‡æ»¤æ¨èç³»ç»Ÿ")
    print("=" * 50)

    # ç¡®å®šæ•°æ®ç›®å½•è·¯å¾„
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(current_dir))
    data_dir = os.path.join(project_root, 'data')

    # åˆå§‹åŒ–ç³»ç»Ÿ
    ncf = NeuralCollaborativeFiltering(data_dir)
    
    # åŠ è½½æ•°æ®
    ncf.load_data()
    
    # å‡†å¤‡æ•°æ®é›†
    ncf.prepare_datasets()
    
    # æ„å»ºæ¨¡å‹
    ncf.build_model()
    
    # è®­ç»ƒæ¨¡å‹
    ncf.train_model(epochs=15)
    
    # ä¿å­˜æ¨¡å‹
    ncf.save_model()
    
    # æµ‹è¯•æ¨è
    print("ğŸ¯ æµ‹è¯•æ¨èç”Ÿæˆ...")
    user_activity = ncf.ratings_df['user_id'].value_counts()
    test_user = user_activity.index[0]
    
    recommendations = ncf.get_recommendations(test_user, 5)
    
    print(f"\nä¸ºç”¨æˆ· {test_user} çš„æ¨è:")
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. å•†å“: {rec['item_id']} | é¢„æµ‹è¯„åˆ†: {rec['predicted_rating']:.3f}")
    
    print("\nğŸ‰ ç¥ç»ååŒè¿‡æ»¤è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()

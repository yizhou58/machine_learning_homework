"""
Amazon Electronics æ•°æ®é¢„å¤„ç†æ¨¡å—
å¤„ç†è¯„åˆ†æ•°æ®å’Œå•†å“å…ƒæ•°æ®ï¼Œä¸ºæ¨èç³»ç»Ÿå»ºæ¨¡åšå‡†å¤‡
"""
import pandas as pd
import numpy as np
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    def __init__(self, data_dir="../../data"):
        self.data_dir = data_dir
        self.ratings_df = None
        self.metadata_df = None
        self.processed_ratings = None
        self.user_item_matrix = None
        
    def load_data(self):
        """åŠ è½½åŸå§‹æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        
        # åŠ è½½è¯„åˆ†æ•°æ®
        ratings_file = f"{self.data_dir}/ratings_Electronics.csv"
        self.ratings_df = pd.read_csv(
            ratings_file, 
            names=['user_id', 'item_id', 'rating', 'timestamp']
        )
        
        # åŠ è½½å•†å“å…ƒæ•°æ®
        metadata_file = f"{self.data_dir}/meta_Electronics.json"
        metadata_list = []
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = eval(line.strip())
                    metadata_list.append({
                        'item_id': item.get('asin', ''),
                        'title': item.get('title', ''),
                        'categories': item.get('categories', []),
                        'price': item.get('price', None),
                        'brand': item.get('brand', ''),
                        'description': item.get('description', '')
                    })
                except:
                    continue
        
        self.metadata_df = pd.DataFrame(metadata_list)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   - è¯„åˆ†æ•°æ®: {len(self.ratings_df):,} æ¡")
        print(f"   - å•†å“å…ƒæ•°æ®: {len(self.metadata_df):,} ä¸ª")
        
    def clean_ratings_data(self):
        """æ¸…æ´—è¯„åˆ†æ•°æ®"""
        print("\nğŸ§¹ æ¸…æ´—è¯„åˆ†æ•°æ®...")
        
        original_size = len(self.ratings_df)
        
        # ç§»é™¤é‡å¤è¯„åˆ†ï¼ˆåŒä¸€ç”¨æˆ·å¯¹åŒä¸€å•†å“çš„å¤šæ¬¡è¯„åˆ†ï¼Œä¿ç•™æœ€æ–°çš„ï¼‰
        self.ratings_df['timestamp'] = pd.to_datetime(self.ratings_df['timestamp'], unit='s')
        self.ratings_df = self.ratings_df.sort_values('timestamp').drop_duplicates(
            subset=['user_id', 'item_id'], keep='last'
        )
        
        # è¿‡æ»¤è¯„åˆ†æ•°æ®ï¼šä¿ç•™è¯„åˆ†æ¬¡æ•°>=5çš„ç”¨æˆ·å’Œè¢«è¯„åˆ†æ¬¡æ•°>=5çš„å•†å“
        user_counts = self.ratings_df['user_id'].value_counts()
        item_counts = self.ratings_df['item_id'].value_counts()
        
        active_users = user_counts[user_counts >= 5].index
        popular_items = item_counts[item_counts >= 5].index
        
        self.processed_ratings = self.ratings_df[
            (self.ratings_df['user_id'].isin(active_users)) &
            (self.ratings_df['item_id'].isin(popular_items))
        ].copy()
        
        print(f"   - åŸå§‹è¯„åˆ†æ•°: {original_size:,}")
        print(f"   - æ¸…æ´—åè¯„åˆ†æ•°: {len(self.processed_ratings):,}")
        print(f"   - æ´»è·ƒç”¨æˆ·æ•°: {self.processed_ratings['user_id'].nunique():,}")
        print(f"   - çƒ­é—¨å•†å“æ•°: {self.processed_ratings['item_id'].nunique():,}")
        
    def create_user_item_matrix(self):
        """åˆ›å»ºç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ"""
        print("\nğŸ“Š åˆ›å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ...")
        
        # åˆ›å»ºç”¨æˆ·å’Œç‰©å“çš„IDæ˜ å°„
        unique_users = self.processed_ratings['user_id'].unique()
        unique_items = self.processed_ratings['item_id'].unique()
        
        self.user_id_map = {user: idx for idx, user in enumerate(unique_users)}
        self.item_id_map = {item: idx for idx, item in enumerate(unique_items)}
        
        # åå‘æ˜ å°„
        self.idx_to_user = {idx: user for user, idx in self.user_id_map.items()}
        self.idx_to_item = {idx: item for item, idx in self.item_id_map.items()}
        
        # æ·»åŠ æ˜ å°„åçš„ID
        self.processed_ratings['user_idx'] = self.processed_ratings['user_id'].map(self.user_id_map)
        self.processed_ratings['item_idx'] = self.processed_ratings['item_id'].map(self.item_id_map)
        
        # åˆ›å»ºç¨€ç–çŸ©é˜µ
        from scipy.sparse import csr_matrix
        
        n_users = len(unique_users)
        n_items = len(unique_items)
        
        self.user_item_matrix = csr_matrix(
            (self.processed_ratings['rating'], 
             (self.processed_ratings['user_idx'], self.processed_ratings['item_idx'])),
            shape=(n_users, n_items)
        )
        
        # è®¡ç®—ç¨€ç–åº¦
        sparsity = 1 - (len(self.processed_ratings) / (n_users * n_items))
        
        print(f"   - çŸ©é˜µç»´åº¦: {n_users:,} x {n_items:,}")
        print(f"   - ç¨€ç–åº¦: {sparsity:.4f} ({sparsity*100:.2f}%)")
        
    def analyze_data_distribution(self):
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        print("\nğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ...")
        
        # è¯„åˆ†åˆ†å¸ƒ
        rating_dist = self.processed_ratings['rating'].value_counts().sort_index()
        print(f"   - è¯„åˆ†åˆ†å¸ƒ:")
        for rating, count in rating_dist.items():
            print(f"     {rating}æ˜Ÿ: {count:,} ({count/len(self.processed_ratings)*100:.1f}%)")
        
        # ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ
        user_activity = self.processed_ratings['user_id'].value_counts()
        print(f"   - ç”¨æˆ·æ´»è·ƒåº¦:")
        print(f"     å¹³å‡è¯„åˆ†æ•°: {user_activity.mean():.1f}")
        print(f"     ä¸­ä½æ•°è¯„åˆ†æ•°: {user_activity.median():.1f}")
        print(f"     æœ€å¤§è¯„åˆ†æ•°: {user_activity.max()}")
        
        # å•†å“çƒ­é—¨åº¦åˆ†å¸ƒ
        item_popularity = self.processed_ratings['item_id'].value_counts()
        print(f"   - å•†å“çƒ­é—¨åº¦:")
        print(f"     å¹³å‡è¢«è¯„åˆ†æ•°: {item_popularity.mean():.1f}")
        print(f"     ä¸­ä½æ•°è¢«è¯„åˆ†æ•°: {item_popularity.median():.1f}")
        print(f"     æœ€å¤§è¢«è¯„åˆ†æ•°: {item_popularity.max()}")
        
    def save_processed_data(self):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        print("\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        
        # ä¿å­˜æ¸…æ´—åçš„è¯„åˆ†æ•°æ®
        self.processed_ratings.to_csv(f"{self.data_dir}/processed_ratings.csv", index=False)
        
        # ä¿å­˜IDæ˜ å°„
        import pickle
        with open(f"{self.data_dir}/user_id_map.pkl", 'wb') as f:
            pickle.dump(self.user_id_map, f)
        with open(f"{self.data_dir}/item_id_map.pkl", 'wb') as f:
            pickle.dump(self.item_id_map, f)
        
        # ä¿å­˜ç”¨æˆ·-ç‰©å“çŸ©é˜µ
        from scipy.sparse import save_npz
        save_npz(f"{self.data_dir}/user_item_matrix.npz", self.user_item_matrix)
        
        print("âœ… æ•°æ®ä¿å­˜å®Œæˆ!")
        
    def run_preprocessing(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...\n")
        
        self.load_data()
        self.clean_ratings_data()
        self.create_user_item_matrix()
        self.analyze_data_distribution()
        self.save_processed_data()
        
        print(f"\nğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print(f"   - æœ€ç»ˆæ•°æ®é›†: {len(self.processed_ratings):,} æ¡è¯„åˆ†")
        print(f"   - ç”¨æˆ·æ•°: {len(self.user_id_map):,}")
        print(f"   - å•†å“æ•°: {len(self.item_id_map):,}")

if __name__ == "__main__":
    preprocessor = DataPreprocessor()
    preprocessor.run_preprocessing()
